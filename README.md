# Project Title: SyriaTel Customer Churn

## 1. Introduction
This project aims to determine the churn rate of new subscribers and identify the best predictors of churning using a dataset extracted from Kaggle.

## 2. Dataset Overview
- Dataset Shape: (3333, 21)
- Columns:
  - ['state', 'account length', 'area code', 'phone number', 'international plan', 'voice mail plan', 'number vmail messages', 'total day minutes', 'total day calls', 'total day charge', 'total eve minutes', 'total eve calls', 'total eve charge', 'total night minutes', 'total night calls', 'total night charge', 'total intl minutes', 'total intl calls', 'total intl charge', 'customer service calls', 'churn']
- Categorical Columns: 
  - ['state', 'phone number', 'international plan', 'voice mail plan']

## 3. Data Preprocessing
- **Data Cleaning**: No missing or duplicate values.
- **Feature Engineering**:
  - Extracted phone area code from 'phone number'.
  - Set 'phone number' as index.
  - Created new features: 
    - Average call duration for different call times.
    - Total charges per call.
    - Customer interaction index.
    - Call charge to minute ratio.
    - Total Activity Index.

## 4. Exploratory Data Analysis (EDA)
- Utilized custom distribution and count plots functions to visualize data distributions.
  - Observed that continuous variables mostly followed a normal distribution, showcasing Gaussian curves.
- Identified insights regarding the distribution patterns of different features and their potential impact on the project's objectives.

## Feature Engineering 2
- Expanded feature set by creating new columns:
  - **Average Call Duration for Different Call Times**:
    
    df['avg_day_call_duration'] = df['total day minutes'] / df['total day calls']
    df['avg_eve_call_duration'] = df['total eve minutes'] / df['total eve calls']
    df['avg_night_call_duration'] = df['total night minutes'] / df['total night calls']
    df['avg_intl_call_duration'] = df['total intl minutes'] / df['total intl calls']
    ```
  - **Total Charges per Call**:
    
    df['day_charge_per_call'] = df['total day charge'] / df['total day calls']
    df['eve_charge_per_call'] = df['total eve charge'] / df['total eve calls']
    df['night_charge_per_call'] = df['total night charge'] / df['total night calls']
    df['intl_charge_per_call'] = df['total intl charge'] / df['total intl calls']
    ```
  - **Customer Interaction Index**:
   
    cols_to_sum = ['customer service calls', 'number vmail messages', 'total day calls', 'total eve calls', 'total night calls', 'total intl calls']
    df['interaction_index'] = df[cols_to_sum].sum(axis=1)
    ```
  - **Call Charge to Minute Ratio**:
    
    df['day_charge_minute_ratio'] = df['total day charge'] / df['total day minutes']
    df['eve_charge_minute_ratio'] = df['total eve charge'] / df['total eve minutes']
    df['night_charge_minute_ratio'] = df['total night charge'] / df['total night minutes']
    df['intl_charge_minute_ratio'] = df['total intl charge'] / df['total intl minutes']
    ```
  - **Total Activity Index**:
    df['total_activity_index'] = df[['total day minutes', 'total eve minutes', 'total night minutes', 'total intl minutes']].sum(axis=1)
    ```
- Addressed missing values generated by these operations.

## Data Splitting
- Split the dataset into 'train_df' and 'test_df' using sklearn for modeling purposes.

## Correlation Analysis
- Employed the chi-squared test for categorical features.
- Conducted point-biserial correlation for continuous values.
- Eliminated features with p-values > 0.05.

## Collinearity Analysis
- Developed a function to handle collinear features.
- Obtained a 'clean_df' with shape (2499, 10) after dropping certain features due to correlation and collinearity.

## Class Imbalance Handling
- Employed SMOTE technique to address class imbalance in the dataset.

## Dimensionality Reduction (PCA)
- Utilized PCA (Principal Component Analysis) for dimensionality reduction.
- Determined the number of components for PCA to be 9.
## Modeling
- Ensured consistency in column numbers between 'train_df' and 'test_df'.
- Utilized various machine learning models:
  - **Logistic Regression**
  - **DecisionTreeClassifier**
  - **KNNs**
  - **Random Forests**
  - **XG-Boost Classifier**
  - **Ensembling**
- Evaluated model performances based on:
  - Accuracy
  - Precision
  - Recall
  - F1-score

## Model Deployment
- Saved the trained model using joblib.
- Created a 'streamlit.py' file to load the 'model.joblib' file and run it with Streamlit.
- To use the app on GitHub:
  1. Clone the repository.
  2. Ensure required Python libraries are installed by running:
     ```
     pip install -r requirements.txt
     ```
  3. Run the Streamlit app using the following command:
     ```
     streamlit run streamlit.py
     ```
  4. Access the app via the provided URL in the terminal.

The 'streamlit.py' file loads the pre-trained model and enables users to interact with the model for predictions or analysis.
